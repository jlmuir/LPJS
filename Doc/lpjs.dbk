<?xml version="1.0" encoding="UTF-8"?>

<book xmlns="http://docbook.org/ns/docbook" version="5.0">
    <title>LPJS - Lightweight, Portable Job Scheduler</title>
    
    <chapter>
        <title>Introduction</title>
        
        <para>
        Small, simple, reliable.  Easy to set up.  Intuitive UI.
        </para>
        
        <para>
        See Research Computing User's Guide.
        </para>
    </chapter>
    
    <chapter>
        <title>Installation</title>
        
        <para>
        Use FreeBSD ports or pkgsrc.  Other package managers may be
        supported by third parties.
        </para>
    </chapter>
    
    <chapter>
        <title>Starting Daemons</title>
        
        <para>
        munged, lpjs_dispatchd, lpjs_compd.  Install munge key on head
        node and all compute nodes.
        </para>
        
        <para>
        Jobs can be submitted from any node with the same version of
        LPJS installed.  It need not be running LPJS daemons, but it
        does require a configuration file point to the head node and munge
        to authenticate requests.  Hence, other computers on the network
        can act as submit nodes, even if they are not part of the cluster/grid.
        </para>
        
        <para>
        Normally run as a service.  Run lpjs-admin and use the menus
        to set up machine as a head node or compute node.  This will
        enable the necessary daemons.
        </para>
        
        <para>
        It is possible to use LPJS without having admin access.
        Start the daemons manually by running <command>munged</command>
        and <command>lpjs_dispatchd</command> on the head node, and
        <command>munged</command> and <command>lpjs_compd</command>
        on each compute node.  Compute nodes will only be able to run jobs
        under the same user name that submitted them.
        </para>
    </chapter>
    
    <chapter>
        <title>Monitoring the Cluster/Grid</title>
        
        <para>
        Learn to do this <emphasis>before</emphasis> running your
        own jobs.
        </para>
    </chapter>
    
    <chapter>
        <title>Running Jobs</title>
        
        <section>
            <title>File Access</title>
            
            <para>
            It is assumed that most compute nodes will have direct access
            to the same files as the submit node, via NFS or similar
            file sharing protocols.  In this case, the LPJS chaperon
            process will run the job on the compute node in the same
            absolute directory from which it was submitted on the submit
            node.
            </para>
            
            <para>
            This is not a requirement, however.  If the working directory
            from which a job was submitted does not exist on a compute node,
            LPJS will create a temporary working directory and run the
            job within it.  It is the job script's responsibility then,
            to download any input files needed, and upload the results
            to the appropriate location (which could be anywhere: This is
            entirely up to the user).  The temporary directory is
            automatically removed upon completion of the job, so scripts
            that fail to transfer the results to a permanent location
            will have to be rerun.
            </para>
            
            <para>
            LPJS is not a file transfer tool.  There are numerous,
            highly-evolved tools for this purpose, such as
            <command>rsync</command>, <command>curl</command>, etc.,
            and no reason to reinvent this wheel.  The system administrator
            of whatever computer is meant to store the results must
            allow file transfers from all compute nodes, using whatever
            transfer tool they choose to support.
            </para>
        </section>
    </chapter>
</book>
